{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 垃圾分类模型\n### 数据集\n出处：https://blog.csdn.net/unique_pei/article/details/105125244\n内容：\n一共有四个大文件夹，对应着不同的垃圾类别，每个文件夹中有各自包含的垃圾名称及其图片，当前数据集一共有246种垃圾，共包含图片80961张\n\n### 模型概述\n#### 问题\n由于数据集类别过多，而很多类别的图片数量较少，个别类别图片较多，导致模型难以收敛\n#### 方案\n训练四个模型，从而降低每个模型的复杂度，提高识别率，和减少收敛时间，每个模型负责识别一个垃圾类别。读取数据时注意数据数量的均等，对数据数量特别大的进行随即丢弃，保证两种数据之间的数量比例大致为1：1。为了提高训练速度，每次直接把所有数据读入，于是每次只读两类的数据，然后进行多次训练从而提高模型的泛化能力。\n"},{"metadata":{},"cell_type":"markdown","source":"## 导入库"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom PIL import Image\nimport os","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 读取数据"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def readimage(path1, path2):\n    '''\n    params:\n        path1: data目录\n        path2: 参照物目录\n    '''\n    data = []\n    data_1 = [] \n    for name in os.listdir(path1):\n        img = Image.open(os.path.join(path1, name))\n        img = np.asarray(img)\n        data.append(img)\n    for name in os.listdir(path2):\n        img = Image.open(os.path.join(path2, name))\n        img = np.asarray(img)\n        data_1.append(img)\n    data = np.asarray(data)\n    data_1 = np.asarray(data_1)\n    \n    # 这个分类数据太多, 进行随机剪切\n    if data.shape[0] > data_1.shape[0]:\n        index = np.arange(data.shape[0])\n        np.random.shuffle(index)\n        data = data[index]\n        data = data[:data_1.shape[0],:,:,:]\n    else:\n        index = np.arange(data_1.shape[0])\n        np.random.shuffle(index)\n        data_1 = data_1[index]\n        data_1 = data_1[:data.shape[0],:,:,:]\n    \n    label = np.ones(data.shape[0])\n    label_1 = np.zeros(data_1.shape[0])\n    \n    print('data:',data.shape,'label:', label.shape, 'data_1:',data_1.shape, 'label_1:', label_1.shape)\n    data = np.concatenate((data,data_1))\n    label = np.concatenate((label,label_1))\n    \n    \n    index=np.arange(len(data))\n    np.random.shuffle(index)\n    data=data[index]\n    label = label[index]\n    print(data.shape, label.shape)\n    return data, label","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 构建模型"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model_part(x, filters):\n    conv1 = layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu')(x)\n    conv1 = layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu')(conv1)\n    conv1 = layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu')(conv1)\n    \n    conv2 = layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu')(x)\n    conv2 = layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu')(conv2)\n    \n    conv3 = layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu')(x)\n    \n    conatenate = layers.concatenate([conv1, conv2, conv3])\n    \n    pool = layers.AveragePooling2D()(conatenate)\n    \n    norm = layers.BatchNormalization()(pool)\n    \n    return norm\n\ndef build_model(name, shape):\n    # (128,128,3)\n    inputs = layers.Input(shape)\n    # (128,128,3) -> (64,64,3)\n    model = build_model_part(inputs, 32)\n    # (64,64,3) -> (32,32,3)\n    model = build_model_part(model, 64)\n    # (32,32,3) -> (16,16,3)\n    model = build_model_part(model, 128)\n    # (16,16,3) -> (8,8,3)\n    model = build_model_part(model, 256)\n\n    # 展平\n    model = layers.Flatten()(model)\n\n    # dropout层\n    model = layers.Dropout(0.5)(model)\n    # 第一层全连接\n    model = layers.Dense(512, activation='relu')(model)\n    # dropout层\n    model = layers.Dropout(0.5)(model)\n    # 第二层全连接\n    model = layers.Dense(64, activation='relu')(model)\n    outputs = layers.Dense(1, activation='sigmoid')(model)\n\n    model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n    \n    return model","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 主函数"},{"metadata":{"trusted":true},"cell_type":"code","source":"def main(name, epochs, batch_size, model_dir):\n    shape = (128,128,3)\n    path = {'kehuishou': '/kaggle/input/resize_image/kehuishou',\n            'youhai': '/kaggle/input/resize_image/youhai',\n            'canyu': '/kaggle/input/resize_image/canyu',\n            'qita': '/kaggle/input/resize_image/qita'}\n    \n    \n    model = build_model(name=name,shape=shape)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    for key, value in path.items():\n        if key == name:\n            continue\n        else:\n            data, label = readimage(path[name], value)\n            model.fit(x=data, y=label, epochs=epochs, validation_split=0.2, batch_size=batch_size)\n            save_path = model_dir + '/' + name\n            file_name = key + '.h5'\n            \n            if not os.path.exists(save_path):\n                os.makedirs(save_path)\n                \n            save_path = os.path.join(save_path, file_name)\n            model.save(save_path)\n            print(key + '数据训练完成')\n            del data, label\n    print('训练结束')","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 运行"},{"metadata":{"trusted":true},"cell_type":"code","source":"main(name='kehuishou', epochs=10, batch_size=128, model_dir='kaggle/working')","execution_count":8,"outputs":[{"output_type":"stream","text":"data: (4833, 128, 128, 3) label: (4833,) data_1: (4833, 128, 128, 3) label_1: (4833,)\n(9666, 128, 128, 3) (9666,)\nTrain on 7732 samples, validate on 1934 samples\nEpoch 1/10\n7732/7732 [==============================] - 25s 3ms/sample - loss: 1.5232 - accuracy: 0.5379 - val_loss: 6.1629 - val_accuracy: 0.4990\nEpoch 2/10\n7732/7732 [==============================] - 23s 3ms/sample - loss: 0.7565 - accuracy: 0.5729 - val_loss: 1.0754 - val_accuracy: 0.5481\nEpoch 3/10\n7732/7732 [==============================] - 23s 3ms/sample - loss: 0.6697 - accuracy: 0.5929 - val_loss: 0.6627 - val_accuracy: 0.6044\nEpoch 4/10\n7732/7732 [==============================] - 23s 3ms/sample - loss: 0.6317 - accuracy: 0.6372 - val_loss: 0.6913 - val_accuracy: 0.5589\nEpoch 5/10\n7732/7732 [==============================] - 23s 3ms/sample - loss: 0.6169 - accuracy: 0.6606 - val_loss: 0.6351 - val_accuracy: 0.6350\nEpoch 6/10\n7732/7732 [==============================] - 23s 3ms/sample - loss: 0.5862 - accuracy: 0.6862 - val_loss: 0.7541 - val_accuracy: 0.5770\nEpoch 7/10\n7732/7732 [==============================] - 23s 3ms/sample - loss: 0.5763 - accuracy: 0.6945 - val_loss: 0.7686 - val_accuracy: 0.5315\nEpoch 8/10\n7732/7732 [==============================] - 23s 3ms/sample - loss: 0.5502 - accuracy: 0.7128 - val_loss: 0.8697 - val_accuracy: 0.5476\nEpoch 9/10\n7732/7732 [==============================] - 23s 3ms/sample - loss: 0.5328 - accuracy: 0.7303 - val_loss: 0.5931 - val_accuracy: 0.6774\nEpoch 10/10\n7732/7732 [==============================] - 23s 3ms/sample - loss: 0.5131 - accuracy: 0.7394 - val_loss: 0.6582 - val_accuracy: 0.6572\n","name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"Unable to create file (unable to open file: name = 'kaggle/working/kehuishou/youhai.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-eaf7d5bbf818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kehuishou'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kaggle/working'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-1ec47751846f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(name, epochs, batch_size, model_dir)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'数据训练完成'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \"\"\"\n\u001b[1;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1008\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 112\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'kaggle/working/kehuishou/youhai.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}